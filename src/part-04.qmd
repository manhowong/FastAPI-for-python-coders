---
title: "Building the LLM Chat Application"
format:
  html:
    toc: true
    code-fold: false
---

# Building the LLM Chat Application

We have arrived at the capstone. You have learned to accept data over the wire, validate it with Pydantic, handle errors gracefully, and juggle slow operations without blocking your server. Now we will apply every one of these skills to build something that feels like magic: a streaming chat API that talks to a large language model, forwarding words to your browser as fast as the AI generates them.

We will build this in two phases. First, a simple "send a prompt, get a full response" endpoint—easier to debug and perfect for simple clients. Then we will upgrade it to streaming, where words trickle across the network one by one, creating that familiar ChatGPT-typewriter effect. Finally, we will open a door for frontend developers by configuring CORS, allowing a React or Vue application to chat with your server.

## Architecture Overview

Before we write code, picture the journey of a single chat message. Your user types "Tell me a joke" into a web interface. That text travels to your FastAPI server, which acts as a thoughtful concierge. Your server reformats the request, adds the secret API key securely from environment variables, and passes it to OpenAI or Anthropic. When the AI responds, your server does not just dump the whole paragraph back; it can choose to either hand over the complete text at once, or relay words as they arrive.

```mermaid
%%{init: {'theme': 'handDrawn'}}%%
flowchart LR
    A[Browser/Frontend<br/>Chat Interface] -->|HTTP POST /chat| B[Your FastAPI App<br/>Validation & Logic]
    B -->|Async API Call<br/>with Secret Key| C[LLM Provider<br/>OpenAI/Anthropic]
    C -->|Streaming Tokens| B
    B -->|JSON or<br/>SSE Stream| A
    
    style A fill:#e8dcc4,stroke:#000
    style B fill:#d8e2dc,stroke:#000
    style C fill:#fdf1b8,stroke:#000
```

Your application sits in the middle—validating inputs, managing secrets, handling the slow network conversation with the AI, and deciding how to parcel out the response.

## Phase One: The Non-Streaming Endpoint

Let us start with the simpler version: the user sends a message, we wait for the entire LLM response to finish, then we return it as one clean JSON package. This is easier to test in Swagger UI and handles errors more straightforwardly.

First, our Pydantic models. We need a request model describing what the user sends, and a response model guaranteeing what they receive. Notice how we reuse the `ChatMessage` concept—this structures the back-and-forth history:

```python
from pydantic import BaseModel
from typing import List

class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    temperature: float = 0.7  # Controls creativity, 0 to 1

class ChatResponse(BaseModel):
    reply: str
    tokens_used: int
```

Now, the endpoint. We will use the dependency injection pattern from Part 3 to manage our LLM client. For this example, we will use the OpenAI Python client, but the pattern is identical for Anthropic or any other async HTTP client:

```python
from fastapi import FastAPI, Depends, HTTPException
from openai import AsyncOpenAI
import os

app = FastAPI()

# Dependency: configured client
async def get_openai_client():
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    try:
        yield client
    finally:
        # AsyncOpenAI handles cleanup automatically, but explicit is nice
        pass

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(
    request: ChatRequest, 
    client: AsyncOpenAI = Depends(get_openai_client)
):
    try:
        # This is the slow network call we learned to await
        completion = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[msg.model_dump() for msg in request.messages],
            temperature=request.temperature
        )
        
        return ChatResponse(
            reply=completion.choices[0].message.content,
            tokens_used=completion.usage.total_tokens
        )
        
    except Exception as e:
        # Translate any OpenAI error into a clean HTTP 500
        # In production, you might distinguish rate limits (429) vs auth errors (401)
        raise HTTPException(
            status_code=503, 
            detail=f"LLM service unavailable: {str(e)}"
        )
```

Test this in Swagger UI. You send:

```json
{
  "messages": [{"role": "user", "content": "Say hello in French"}],
  "temperature": 0.5
}
```

After a brief wait, you receive the full JSON response. The server was `await`ing the OpenAI call, so during that second or two of waiting, your health check endpoint and other routes remained perfectly responsive.

## Phase Two: Streaming Responses

Now for the upgrade. Waiting for the full response feels sluggish when answers are long. We want words to appear as they are generated, like watching someone type in real time.

In Python, you know about generators—functions that `yield` values one by one instead of returning everything at once. We will use an async generator (`async for`) to receive tokens from OpenAI, and FastAPI’s `StreamingResponse` to pipe them to the browser using a format called **Server-Sent Events** (SSE).

Think of a normal `return` as handing over a sealed envelope. Streaming is like reading a letter aloud over the phone, sentence by sentence, while you are still receiving it.

Here is the streaming endpoint. Notice how it shares the same dependency injection and error handling concepts, but changes the return type:

```python
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI

async def generate_chat_stream(messages, client: AsyncOpenAI):
    """Async generator that yields text chunks as they arrive from OpenAI."""
    try:
        stream = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[msg.model_dump() for msg in messages],
            stream=True  # The magic word
        )
        
        # This loop runs as the AI generates each token
        async for chunk in stream:
            if content := chunk.choices[0].delta.content:
                # Yield the text chunk formatted as SSE
                yield f"data: {content}\n\n"
                
        # Signal that the stream is complete
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        # Even streams need error handling
        yield f"data: ERROR: {str(e)}\n\n"

@app.post("/chat/stream")
async def chat_stream_endpoint(
    request: ChatRequest,
    client: AsyncOpenAI = Depends(get_openai_client)
):
    return StreamingResponse(
        generate_chat_stream(request.messages, client),
        media_type="text/event-stream"
    )
```

The `media_type="text/event-stream"` tells the browser: "This is not a JSON file; this is a live feed that keeps coming." Each `yield` sends a chunk immediately over the wire.

To test this, use `curl` in your terminal (browsers handle SSE specially):

```bash
curl -X POST http://localhost:8000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Count to 10 slowly"}], "temperature": 0.5}'
```

You will see numbers appear one by one, with slight delays between them, rather than all at once after ten seconds.

## CORS: Opening the Door to Frontend Developers

Your API works beautifully when tested with `curl` or Swagger UI, but there is a security guard standing between web browsers and your server. The **Same-Origin Policy** prevents a web page served from `localhost:3000` (your React app) from talking to `localhost:8000` (your FastAPI app) unless your server explicitly grants permission.

Think of CORS (Cross-Origin Resource Sharing) as a bouncer at a club. By default, he blocks anyone from a different neighborhood. We need to hand him a list saying "Let in anyone from localhost:3000" (during development) or "Let in my official frontend domain" (in production).

FastAPI makes this one line of configuration using middleware—a wrapper that runs around every request:

```python
from fastapi.middleware.cors import CORSMiddleware

# Add this right after creating your app instance
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],  # Vite and Create React App defaults
    allow_credentials=True,
    allow_methods=["*"],  # GET, POST, etc.
    allow_headers=["*"],  # Content-Type, etc.
)
```

Now your frontend team can build a chat interface in React, Vue, or vanilla JavaScript, and it will communicate seamlessly with your streaming endpoint.

## The Capstone Project: A Working Chat API

Let us wire everything together into a complete, productionish example. We will add in-memory conversation history so the AI remembers context across messages, and we will structure the code cleanly.

Create `chat_app.py`:

```python
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import AsyncOpenAI
from typing import List
import os
import asyncio

app = FastAPI()

# CORS for frontend developers
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Be restrictive in production!
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    temperature: float = 0.7

# In-memory storage (use Redis or database in production)
conversation_history: List[ChatMessage] = []

# Dependencies
async def get_llm_client():
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    yield client

# Non-streaming: reliable and simple
@app.post("/chat")
async def chat(
    request: ChatRequest,
    client: AsyncOpenAI = Depends(get_llm_client)
):
    try:
        # Add user message to history
        conversation_history.extend(request.messages)
        
        completion = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[m.model_dump() for m in conversation_history],
            temperature=request.temperature
        )
        
        reply = completion.choices[0].message.content
        conversation_history.append(ChatMessage(role="assistant", content=reply))
        
        return {"reply": reply, "history_length": len(conversation_history)}
        
    except Exception as e:
        raise HTTPException(status_code=503, detail=str(e))

# Streaming: the polished UX
@app.post("/chat/stream")
async def chat_stream(
    request: ChatRequest,
    client: AsyncOpenAI = Depends(get_llm_client)
):
    async def event_generator():
        stream = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[m.model_dump() for m in request.messages],
            stream=True
        )
        
        full_response = []
        async for chunk in stream:
            if content := chunk.choices[0].delta.content:
                full_response.append(content)
                yield f"data: {content}\n\n"
        
        # Store complete message in history after streaming
        conversation_history.append(
            ChatMessage(role="assistant", content="".join(full_response))
        )
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/history")
def get_history():
    """See what the AI remembers"""
    return {"messages": conversation_history}
```

You now have a complete system: validated inputs, dependency-injected LLM client, streaming and non-streaming options, CORS-enabled for frontends, graceful error handling, and conversation memory.

## Testing and Next Steps

Before you ship this, write a quick test using FastAPI’s `TestClient`. It runs your app without starting the real server, perfect for CI/CD pipelines:

```python
from fastapi.testclient import TestClient

client = TestClient(app)

def test_chat_endpoint():
    response = client.post("/chat", json={
        "messages": [{"role": "user", "content": "Hi"}],
        "temperature": 0.5
    })
    assert response.status_code == 200
    assert "reply" in response.json()
```

For deployment, remember that `async` scales beautifully, but you still need to handle:
- **Lifespan events**: Use `@app.on_event("startup")` or the modern `contextlib.asynccontextmanager` lifespan to initialize your database or LLM client once, not per request.
- **Real databases**: Replace that `conversation_history` list with PostgreSQL and an async ORM like SQLModel or Prisma Client Python.
- **Secrets management**: Move from `.env` files to proper secret managers like AWS Secrets Manager or HashiCorp Vault in production.

You have built a foundation that serves equally well for inventory APIs, data dashboards, or AI applications. The patterns—Pydantic for contracts, `async` for concurrency, dependencies for resource management—are the modern standard for Python web services.

## Part Recap: Terms You Learned

You have crossed the threshold from tutorial follower to API architect. Here is the final vocabulary set:

| Web Term | What it actually means in Python |
|----------|----------------------------------|
| **StreamingResponse** | A special return type that lets you `yield` data incrementally over a persistent HTTP connection |
| **Server-Sent Events (SSE)** | The protocol format (`data: ...\n\n`) for sending a stream of text updates from server to browser |
| **CORS** | The browser security system that blocks cross-domain requests unless explicitly allowed via middleware |
| **Generator (`yield`)** | A function that produces values one at a time, pausing between each, perfect for streaming chunks |
| **Async Generator (`async for`)** | A generator that can await slow operations (like LLM tokens) between yields |
| **Lifespan Events** | Hooks for running setup (connect to DB) and teardown (cleanup) around your server’s life |

Your FastAPI journey is complete. You began thinking of the web as remote function calls, learned to validate and structure data with Pydantic, mastered the async juggling act, and finally connected to the slow, wondrous world of AI APIs. Whatever you build next—chatbots, data pipelines, or mobile backends—you have the tools to make it fast, reliable, and clean.
